<div class="container">
  <h2>Papers</h2>
  <div id="pubs">
<!--
		<div class="pubwrap">
			<img src="/img/thesis.jpg" style="width:100%">
			<div class="pubd" style="display:inline-block"><a href="http://cs.stanford.edu/people/karpathy/main.pdf">[PDF]</a> Connecting Images and Natural Language, </div>
			<div class="puba" style="display:inline-block">Andrej Karpathy, </div>
			<div class="pubv" style="display:inline-block">PhD Thesis, 2016</div>
		</div>

    <div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">
            <img src="/img/densecap.png">
          </div>
        </div>
        <div class="col-md-6">
          <div class="pub">
            <div class="pubt">DenseCap: Fully Convolutional Localization Networks for Dense Captioning</div>
            <div class="pubd">Efficiently identify and caption all the things in an image with a single forward pass of a network. Our model is fully differentiable and trained end-to-end without any pipelines. The model is also very efficient (processes a 720x600 image in only 240ms), and evaluation on a large-scale dataset of 94,000 images and 4,100,000 region captions shows that it outperforms baselines based on previous approaches.</div>
            <div class="puba">Justin Johnson*, Andrej Karpathy*, Li Fei-Fei</div>
            <div class="pubv">CVPR 2016 (Oral)</div>
            <div class="publ">
              <ul>
                <li><a href="http://cs.stanford.edu/people/karpathy/densecap">Project</a></li>
                <li><a href="http://cs.stanford.edu/people/karpathy/densecap.pdf">PDF</a></li>
                <li><a href="https://github.com/jcjohnson/densecap">Code (Github)</a></li>
              </ul>
            </div>
          </div>
        </div>
      </div>
      <div class="row">
        <img src="/img/densecap_more.jpg" style="max-width:100%; margin-top:30px;">
      </div>
    </div>
-->
    <div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">
            <img src="/img/icon.jpg">
          </div>
        </div>
        <div class="col-md-6">
          <div class="pub">
            <div class="pubt">State Interpretation for Social Hierarchical Learning</div>
            <div class="pubd">The conclusion of my work at Scazlab during the summer of 2016, this paper discusses how I assisted Dr. Olivier Manguin's and Dr. Alessandro Roncone's research on enabling effective human robot collaboration. The broader purpose of this specific research was transitioning a robot from the role of student, to peer, and then to teacher. My own efforts went towards enabling our robot to understand the board state of a childrens game called snap circuits. </div>
            <div class="puba">George Wildridge</div>
            <div class="pubv">Choate Rosemary Hall</div>
            <div class="publ">
              <ul>
                <li><a href="https://github.com/georgewildridge/georgewildridge.github.io/raw/master/papers/final-writeup.pdf" target="_blank">PDF</a></li>
								<li><a href="http://github.com/georgewildridge" target="_blank">Code (GitHub)</a></li>
                <!-- <li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">Blog Post</a></li>
								<li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">Presentation</a></li> -->
              </ul>
            </div>
          </div>
        </div>
      </div>
    </div>


      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">
          <img src="/img/rnn7.png">
          </div>
        </div>
        <div class="col-md-6">
          <div class="pub">
            <div class="pubt">A Review of Social Heirarchical Learning</div>
            <div class="pubd">Social Heirarchical Learning aims to enable effective human robot collaboration and was the basis of Dr. Bradley Hayes's research while under Professor Scassellati at the Social Robotics Lab. After working under Brad for the previous two summers, I had the oppourtunity to fully explore his work through the Science Research Program at Choate. To write this paper, I read everything from review articles on the field of artificial intelligence and machine learning, to review articles on the specific subsets of the forementioned fields, such as hierarchical leraning, to all of Brad's papers from his time at Yale. In addition, I also spent a considerable amount of time going through Berkeley's Artifical Intelligence course to ensure a comprehensive understanding of the ideas I discuss. Although Brad moved on to do great things at MIT, I had the oppourtunity to continue in Professor Brian Scassellati's lab for a third summer under Dr. Olivier Mangin and Dr. Alessandro Roncone, who picked up where Brad's research left off. 
           <!-- <a href="http://www.nytimes.com/2014/11/18/science/researchers-announce-breakthrough-in-content-recognition-software.html">New York Times article</a> -->.</div>
            <div class="puba">George Wildridge</div>
            <div class="pubv">Choate Rosemary Hall</div>
            <div class="publ">
              <ul>
                <li><a href=https://github.com/georgewildridge/georgewildridge.github.io/raw/master/papers/social-heirarchical-learning.pdf target="_blank">PDF</a></li>
                <li><a href="https://github.com/georgewildridge" target="_blank">Code (Github)</a></li>
               <!--  <li><a href="http://cs.stanford.edu/people/karpathy/deepimagesent/rankingdemo">Blog Post</a></li> -->
              </ul>
            </div>
          </div>
        </div>
      </div>
    </div>
<!--
    <div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">
          <img src="/img/mosaic_sm.jpg">
          </div>
        </div>
        <div class="col-md-6">
          <div class="pub">
            <div class="pubt">ImageNet Large Scale Visual Recognition Challenge</div>
            <div class="pubd">Everything you wanted to know about ILSVRC: data collection, results, trends, current computer vision accuracy, even a stab at computer vision vs. human vision accuracy -- all here! My own contribution to this work were the <a href="http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/">human accuracy evaluation experiments</a>.
            </div>
            <div class="puba">Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, Li Fei-Fei</div>
            <div class="pubv">IJCV 2015</div>
            <div class="publ">
              <ul>
                <li><a href="http://arxiv.org/abs/1409.0575">PDF</a></li>
                <li><a href="http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/">Blog Post</a></li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">
          <img src="/img/fragimg3.png">
          </div>
        </div>
        <div class="col-md-6">
          <div class="pub">
            <div class="pubt">Deep Fragment Embeddings for Bidirectional Image-Sentence Mapping</div>
            <div class="pubd">We train a multi-modal embedding to associate fragments of images (objects) and sentences (noun and verb phrases) with a structured, max-margin objective. Our model enables efficient and interpretible retrieval of images from sentence descriptions (and vice versa).</div>
            <div class="puba">Andrej Karpathy, Armand Joulin, Li Fei-Fei</div>
            <div class="pubv">NIPS 2014</div>
            <div class="publ">
              <ul>
                <li><a href="http://cs.stanford.edu/people/karpathy/nips2014.pdf">PDF</a></li>
                <li><a href="http://cs.stanford.edu/people/karpathy/nips2014_supp.pdf">Supplemental</a></li>
                <li><a href="http://cs.stanford.edu/people/karpathy/defrag/index.html">Code</a></li>
                <li><a href="http://cs.stanford.edu/people/karpathy/defragvis/">Web Demo of Results</a></li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">
            <img src="/img/videonets.png">
          </div>
        </div>
        <div class="col-md-6">
          <div class="pub">
            <div class="pubt">Large-Scale Video Classification with Convolutional Neural Networks</div>
            <div class="pubd">We introduce Sports-1M: a dataset of 1.1 million YouTube videos with 487 classes of Sport. This dataset allowed us to train large Convolutional Neural Networks that learn spatio-temporal features from video rather than single, static images.</div>
            <div class="puba">Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, Li Fei-Fei</div>
            <div class="pubv">CVPR 2014 (Oral)</div>
            <div class="publ">
              <ul>
                <li><a href="http://cs.stanford.edu/people/karpathy/deepvideo/">Project</a></li>
                <li><a href="http://cs.stanford.edu/people/karpathy/deepvideo/deepvideo_cvpr2014.pdf">PDF</a></li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">
            <img src="/img/rnn.png">
          </div>
        </div>
        <div class="col-md-6">
          <div class="pub">
            <div class="pubt">Grounded Compositional Semantics for Finding and Describing Images with Sentences</div>
            <div class="pubd"> Our model learns to associate images and sentences in a common

              We use a Recursive Neural Network to compute representation for sentences and a Convolutional Neural Network for images. We then learn a model that associates images and sentences through a structured, max-margin objective. </div>
            <div class="puba">Richard Socher, Andrej Karpathy, Quoc V. Le, Christopher D. Manning, Andrew Y. Ng</div>
            <div class="pubv">TACL 2013</div>
            <div class="publ">
              <ul>
                <li><a href="http://nlp.stanford.edu/~socherr/SocherKarpathyLeManningNg_TACL2013.pdf">PDF</a></li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">
            <img src="/img/discovernet.png">
          </div>
        </div>
        <div class="col-md-6">
          <div class="pub">
            <div class="pubt">Emergence of Object-Selective Features in Unsupervised Feature Learning</div>
            <div class="pubd">
              We introduce an unsupervised feature learning algorithm that is trained explicitly with k-means for simple cells and a form of agglomerative clustering for complex cells. When trained on a large dataset of YouTube frames, the algorithm automatically discovers semantic concepts, such as faces.
            </div>
            <div class="puba">Adam Coates, Andrej Karpathy, Andrew Ng</div>
            <div class="pubv">NIPS 2012</div>
            <div class="publ">
              <ul>
                <li><a href="http://cs.stanford.edu/people/karpathy/nips2012.pdf">PDF</a></li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="pubwrap" style="border-bottom: none; padding-bottom:0px; margin-bottom:30px;">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">
            <img src="/img/quadruped.png">
          </div>
        </div>
        <div class="col-md-6">
          <div class="pub">
            <div class="pubt">Locomotion Skills for Simulated Quadrupeds</div>
            <div class="pubd">We develop an integrated set of gaits and skills for a physics-based simulation of a quadruped. The controllers use a representation based on gait graphs, a dual leg frame model, a flexible spine model, and the extensive use of internal virtual forces applied via the Jacobian transpose.</div>
            <div class="puba">Stelian Coros, Andrej Karpathy, Benjamin Jones, Lionel Reveret, Michiel van de Panne</div>
            <div class="pubv">SIGGRAPH 2011</div>
            <div class="publ">
              <ul>
                <li><a href="http://www.cs.ubc.ca/~van/papers/2011-TOG-quadruped/index.html">Project</a></li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="showmore" id="showmorepubs">
    show more
    </div>

    <div id="morepubs">


      <div class="pubwrap">
        <div class="row">
          <div class="col-md-6">
            <div class="pubimg">
              <img src="/img/discovery.jpg">
            </div>
          </div>
          <div class="col-md-6">
            <div class="pub">
              <div class="pubt">Object Discovery in 3D scenes via Shape Analysis</div>
              <div class="pubd">Wouldn't it be great if our robots could drive around our environments and autonomously discovered and learned about objects? In this work we introduce a simple object discovery method that takes as input a scene mesh and outputs a ranked set of segments of the mesh that are likely to constitute objects.</div>
              <div class="puba">Andrej Karpathy, Stephen Miller, Li Fei-Fei</div>
              <div class="pubv">ICRA 2013</div>
              <div class="publ">
                <ul>
                  <li><a href="http://cs.stanford.edu/~karpathy/discovery/">PDF, Code, Data</a></li>
                </ul>
              </div>
            </div>
          </div>
        </div>
      </div>

      <div class="pubwrap" style="border-bottom: none;">
        <div class="row">
          <div class="col-md-6">
            <div class="pubimg">
              <img src="/img/curriculum.png">
            </div>
          </div>
          <div class="col-md-6">
            <div class="pub">
              <div class="pubt">Curriculum Learning for Motor Skills </div>
              <div class="pubd">
                My UBC Master's thesis project. My work was on curriculum learning for motor skills. In particular, I was working with a heavily underactuated (single joint) footed acrobot. The acrobot used a devised curriculum to learn a large variety of parameterized motor skill policies, skill connectivites, and also hierarchical skills that depended on previously acquired skills. Almost all of it from scratch. The project was heavily influenced by intuitions about human development and learning (i.e. trial and error learning, the idea of gradually building skill competencies). The ideas in this work were good, but at the time I wasn't savvy enough to formulate them in a mathematically elaborate way. The video is a fun watch!
              </div>
              <div class="puba">Andrej Karpathy, Michiel van de Panne</div>
              <div class="pubv">AI 2012</div>
              <div class="publ">
                <ul>
                  <li><a href="http://www.cs.ubc.ca/~van/papers/2012-AI-curriculum/index.html">Project</a></li>
                  <li><a href="http://www.cs.ubc.ca/~van/papers/2012-AI-curriculum/2012-AI-curriculum.pdf">PDF</a></li>
                  <li><a href="http://vimeo.com/24446828" savefrom_lm_index="0" savefrom_lm="1">Video</a><span style="padding: 0; margin: 0; margin-left: 5px;"><a href="http://savefrom.net/?url=http%3A%2F%2Fvimeo.com%2F24446828&amp;utm_source=userjs-chrome&amp;utm_medium=extensions&amp;utm_campaign=link_modifier" target="_blank" title="Get a direct link" savefrom_lm="1" savefrom_lm_is_link="1" style="background-image: url(&quot;data:image/gif;base64,R0lGODlhEAAQAOZ3APf39+Xl5fT09OPj4/Hx8evr6/3+/u7u7uDh4OPi497e3t7e3/z8/P79/X3GbuXl5ubl5eHg4WzFUfb39+Pj4lzGOV7LOPz7+/n6+vn5+ZTLj9/e387Ozt7f3/7+/vv7/ISbePn5+m/JV1nRKXmVbkCnKVrSLDqsCuDh4d/e3uDn3/z7/H6TdVeaV1uSW+bn5v39/eXm5eXm5kyHP/f39pzGmVy7J3yRd9/f3mLEKkXCHJbka2TVM5vaZn6Wdfn6+YG/c/r5+ZO/jeLi41aHTIeageLn4f39/vr6+kzNG2PVM5i+lomdf2CXYKHVmtzo2YXNeDqsBebl5uHh4HDKWN3g3kKqEH6WeZHTXIPKdnSPbv79/pfmbE7PHpe1l4O8dTO5DODg4VDLIlKUUtzo2J7SmEWsLlG4NJbFjkrJHP7+/VK5Nfz8+zmnC3KKa+Hg4OHh4Y63j/3+/eDg4Ojo6P///8DAwP///wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACH5BAEAAHcALAAAAAAQABAAAAfWgHd2g4SFhYJzdYqLjIpzgx5bBgYwHg1Hk2oNDXKDFwwfDF5NLmMtcStsn4MhGT8YS04aGmU1QRhIGYMTADQAQlAODlloAMYTgwICRmRfVBISIkBPKsqDBAREZmcVFhYVayUz2IMHB1dWOmImI2lgUVrmgwUFLzdtXTxKSSduMfSD6Aik48MGlx05SAykM0gKhAAPAhTB0oNFABkPHg5KMIBCxzlMQFQZMGBIggSDpsCJgGDOmzkIUCAIM2dOhEEcNijQuQDHgg4KOqRYwMGOIENIB90JBAA7&quot;); background-repeat: no-repeat; width: 16px; height: 16px; display: inline-block; border: none; text-decoration: none; padding: 0px; position: relative;"></a></span></li>
                </ul>
              </div>
            </div>
          </div>
        </div>
      </div>
-->

    </div>


  </div>
